import bert_score
from rouge_score import rouge_scorer


def score_each(hyps, refs, metric="bertscore", model_type="microsoft/deberta-xlarge-mnli"):
    """
    Compute the bert score or rough score for hypothesis and reference pairs.

    Args:
        hyps: a list of string, hypothesis
        refs: a list of string, references
        metric: metric to compute, bertsocre, rouge1, rouge2, rougeL, rougeLsum
        model_type: model to cacluate bertscore

    Returns:
        precisions, recalls, fmeasures
    """

    if metric == "bertscore":
        precisions, recalls, fmeasures = bert_score.score(hyps, refs, model_type=model_type, lang="en", verbose=True)
        return precisions.tolist(), recalls.tolist(), fmeasures.tolist()
    elif metric in ["rouge1", "rouge2", "rougeL", "rougeLsum"]:
        scorer = rouge_scorer.RougeScorer([metric])
        precisions, recalls, fmeasures = [], [], []
        # for each of the hypothesis and reference documents pair
        for (h, r) in zip(hyps, refs):
            # computing the ROUGE
            score = scorer.score(h, r)
            # separating the measurements
            precision, recall, fmeasure = score[metric]
            precisions.append(precision)
            recalls.append(recall)
            fmeasures.append(fmeasure)
        return precisions, recalls, fmeasures
    else:
        raise ValueError('Metric is not implemented.')


def score(hyps, ref, metric="bertscore", model_type="microsoft/deberta-xlarge-mnli", lang="en"):
    """
    Compute the bert score or rough score given a gold summary and a list of summaries generated by models.

    Args:
        hyps: a list of summaries generated by models
        ref: a gold summary
        metric: metric to compute, bertsocre, rouge1, rouge2, rougeL, rougeLsum
        model_type: model to cacluate bertscore

    Returns:
        precisions, recalls, fmeasures
    """
    refs = [ref] * len(hyps)
    return score_each(hyps, refs, metric, model_type)


def visualize_bert_score(hyps, ref, rescale_with_baseline=True, model_type="microsoft/deberta-xlarge-mnli"):
    """
    Visualize the bert score given a gold summary and a list of summaries generated by models.

    Args:
        hyps: a list of summaries generated by models
        ref: a gold summary
        rescale_with_baseline: metric to compute, bertsocre, rouge1, rouge2, rougeL, rougeLsum
        model_type: model to cacluate contextualized embeddings
    """

    for hyp in hyps:
        bert_score.plot_example(hyp, ref, model_type=model_type, lang="en", rescale_with_baseline=rescale_with_baseline)
